<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="Explained how dinov3 works" />
        <meta name="author" content="Vitaly" />
        <title>DINOv3 Post</title>
        <!-- Favicon-->
        <link rel="icon" type="image/x-icon" href="../assets/favicon.ico" />
        <!-- Custom Google font-->
        <link rel="preconnect" href="https://fonts.googleapis.com" />
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
        <link href="https://fonts.googleapis.com/css2?family=Plus+Jakarta+Sans:wght@100;200;300;400;500;600;700;800;900&amp;display=swap" rel="stylesheet" />
        <!-- Bootstrap icons-->
        <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.8.1/font/bootstrap-icons.css" rel="stylesheet" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="../css/styles.css" rel="stylesheet" />
    </head>
    <body class="d-flex flex-column h-100 bg-light">
        <main class="flex-shrink-0">
            <!-- Navigation-->
            <nav class="navbar navbar-expand-lg navbar-light bg-white py-3">
                <div class="container px-5">
                    <a class="navbar-brand" href="../index.html"><span class="fw-bolder text-primary">Neural Paw</span></a>
                    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
                    <div class="collapse navbar-collapse" id="navbarSupportedContent">
                        <ul class="navbar-nav ms-auto mb-2 mb-lg-0 small fw-bolder">
                            <li class="nav-item"><a class="nav-link" href="../index.html">Home</a></li>
                            <li class="nav-item"><a class="nav-link" href="../resume.html">Resume</a></li>
                            <li class="nav-item"><a class="nav-link" href="../projects.html">Projects</a></li>
                        </ul>
                    </div>
                </div>
            </nav>
            
            <!-- Post Header -->
            <section class="bg-gradient-primary-to-secondary text-white py-5">
                <div class="container px-5 text-center">
                    <h1 class="display-4 fw-bolder">ü¶ñ DINOv3: The Dinosaur That Ate All of Computer Vision</h1>
                    <p class="lead fw-light">Time to say bye-bye to ResNet? üëã</p>
                </div>
            </section>

            <!-- Post Content -->
            <section class="py-5 bg-light">
                <div class="container px-5">
                    <div class="row justify-content-center">
                        <div class="col-xxl-8 blog-post">
                            <div class="mb-4">
                                <img src="../assets/dino_paper.jpg" class="img-fluid rounded-4 mb-4" alt="Dino illustration">

                                <p>
                                    <strong>Self-supervised learning</strong> (SSL) has long promised freedom from manual labels.  
                                    Instead of begging humans to annotate cats and dogs, SSL teaches models to <b>learn directly from raw pixels</b>.  
                                    <a href="https://arxiv.org/abs/2508.10104" target="_blank">DINOv3</a> is the latest big step in this journey and honestly, 
                                    it's kind of a <b>M O N S T E R</b> (in a good way of course).
                                </p>

                                <h3 style="margin-top:2rem;margin-bottom:1rem;">ü§ì Why DINOv3 Matters</h3>
                                <p>
                                    Unlike task-specific models, DINOv3 is built as a <b>universal visual encoder</b>.  
                                    It learns rich global features (great for classification) and precise local features (essential for segmentation, depth, tracking).  
                                    That's a rare combo, because previous models usually had to pick one side.
                                </p>
                                
                                <div style="text-align: center;">
                                    <img 
                                        src="https://images.unsplash.com/photo-1468091730376-d3b5558b71ab?q=80&w=2710&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
                                        alt="One way unsplash" 
                                        style="max-width:100%; border-radius:12px;"
                                    >
                                    <p style="font-size: 14px; color: gray;">Typical task-specific model.</p>
                                </div>
                                    
                                <p>
                                    DINOv3 has a <strong>7B PARAMETERS!</strong> And training a such vision model at the <strong>7B parameter scale</strong> is not only about raw compute.
                                    It surfaces a number of stability and scalability problems that researchers had to carefully solve. 
                                    Let's break down the key innovations that made DINOv3 possible.
                                </p>
                                
                                <h5 style="margin-top:2rem;margin-bottom:1rem;">üß≤ Gram Anchoring: Saving Dense Features!</h5>
                                <p>
                                    <b>üö® Problem:</b> When training the huge 7B DINOv3 model for a long time, global metrics (like classification) keep improving as expected. 
                                          But dense prediction tasks, like segmentation, start degrading after ~200k iterations. 
                                          Why? ü§î Patch-level features begin to ‚Äúblur‚Äù - the model loses local detail, and similarity maps become noisy and less reliable.
                                </p>
                                <p>
                                    <b>‚úÖ Solution:</b> The idea is simple but clever: instead of forcing the patch features to stay in place directly, we preserve <strong>their relationships</strong>. 
                                          We do this using the <strong>Gram matrix</strong> - a matrix of all pairwise dot products of patch features in an image.</p>

                                    <p>We take a <strong>Gram teacher</strong> - an early snapshot of the model with high-quality dense features - and push the student model to match its Gram matrix:</p>

                                    \[
                                    L_{\text{Gram}} = \left\| X_S X_S^\top - X_G X_G^\top \right\|_F^2
                                    \]

                                    <ul>
                                        <li><strong>X<sub>S</sub></strong> ‚Äî L2-normalized patch features of the student</li>
                                        <li><strong>X<sub>G</sub></strong> ‚Äî L2-normalized patch features of the Gram teacher</li>
                                    </ul>
                                    <p>This way, the features themselves are free to move, but the <em>structure of patch similarities remains intact</em>. 
                                </p>

                                <div style="text-align: center;">
                                    <img 
                                        src="../assets/dino_noise_examplepng.png"
                                        alt="Noise" 
                                        style="max-width:100%; border-radius:12px;"
                                    >
                                    <p style="font-size: 14px; color: gray;">Change in cosine similarity between the red-marked patch and all other patches over training. 
                                        As training continues, the model's features lose spatial localization, causing the similarity maps to become increasingly noisy.
                                    </p>
                                </div>
                                
                                <h5 style="margin-top:2rem;margin-bottom:1rem;">üåç Massive Data Curation</h5>
                                <p>
                                    <b>üö® Problem:</b> Scaling models blindly on raw web data can lead to noisy, unbalanced datasets that harm 
                                    generalization. A model of this size needs <em>diverse but clean</em> input to avoid memorization 
                                    and overfitting.  
                                    <br><br>
                                    <b>‚úÖ Solution:</b> The DINOv3 team curated a dataset of <strong>1.6 billion+</strong> images, carefully 
                                    filtered to remove low-quality samples and ensure diversity. This massive yet controlled dataset 
                                    provides the foundation for the model's broad generalization across domains and tasks.
                                </p>
                                
                                 <div style="text-align: center;">
                                    <img 
                                        src="../assets/annotation_problem.png"
                                        alt="Annotation meme" 
                                        style="max-width:100%; border-radius:12px;"
                                    >
                                </div>

                                <h5 style="margin-top:2rem;margin-bottom:1rem;">üìè A Scalable Family of Models</h5>
                                <p>
                                    <b>üö® Problem:</b> While a 7B parameter model is great for research, it's impractical for most real-world 
                                    deployments due to cost and latency.  
                                    <br><br>
                                    <b>‚úÖ Solution:</b> The team distilled the giant model into a whole <strong>family of ViTs</strong>, 
                                    ranging from small (ViT-S, ~21M params) to large (ViT-H+, ~0.8B params). These distilled versions 
                                    inherit most of the power of the 7B model but are efficient enough for everyday use cases.  
                                    This makes DINOv3 both a research breakthrough and a practical tool.
                                </p>
                                
                                <h5 style="margin-top:2rem;margin-bottom:1rem;">üõ† Post-Training Tricks</h5>
                                <p>
                                    <b>üö® Problem:</b> Even after training, models may fail to adapt to higher resolutions, lack zero-shot text 
                                    understanding, or be too rigid for transfer.  
                                    <br><br>
                                    <b>‚úÖ Solution:</b> DINOv3 employs several clever <strong>post-training strategies</strong>:
                                </p>
                                <ul>
                                    <li><strong>Resolution Scaling</strong>: adapts the model to work with high-res inputs (512px, 768px, and even 4K).</li>
                                    <li><strong>Distillation</strong>: compresses knowledge from the huge teacher into smaller, faster students.</li>
                                    <li><strong>Text Alignment</strong>: adds a contrastive training phase that links image features with captions, 
                                    enabling <em>zero-shot multimodal capabilities</em> similar to CLIP but with stronger dense representations.</li>
                                </ul>
                                
                                <p>
                                    Together, these innovations solve the fundamental problems of instability, noisy data, 
                                    impractical scale, and limited adaptability. 
                                </p>
                                <p>
                                    üèÅ<b>The result:</b> a vision foundation model that is not 
                                    only <em>state-of-the-art</em> in research benchmarks but also practical and versatile for real-world 
                                    computer vision applications.
                                </p>

                                
                                <h2 style="margin-top:2rem;margin-bottom:1rem;">üìä Benchmarks</h2>
                                <p>
                                The frozen 7B model already crushes benchmarks.  
                                Add tiny task-specific heads and suddenly you've got <b>state-of-the-art object detection, segmentation, depth estimation</b>, and even 3D view understanding.  
                                All from the same backbone.  
                                </p>

                                <h2 style="margin-top:2rem;margin-bottom:1rem;">üé• The Fun Part: Object Tracking Demo</h2>
                                <p>
                                Alright, enough theory. Here's the cool part: I want to show you how to simply use DINOv3 for <b>video object tracking</b> with just a mouse click.
                                </p>

                                <div style="text-align: center; display: flex; justify-content: center; gap: 20px; flex-wrap: wrap;">
                                    <div>
                                        <img 
                                            src="https://raw.githubusercontent.com/Vitgracer/DinoV3-Object-Tracking/refs/heads/main/results/cat.gif" 
                                            alt="Cat GIF" 
                                            style="max-width:250px; border-radius:12px;"
                                        >
                                        <p style="font-size: 14px; color: gray;">Cat tracking example</p>
                                    </div>

                                    <div>
                                        <img 
                                            src="https://raw.githubusercontent.com/Vitgracer/DinoV3-Object-Tracking/refs/heads/main/results/dog.gif" 
                                            alt="Dog GIF" 
                                            style="max-width:250px; border-radius:12px;"
                                        >
                                        <p style="font-size: 14px; color: gray;">Dog tracking example</p>
                                    </div>

                                    <div>
                                        <img 
                                            src="https://raw.githubusercontent.com/Vitgracer/DinoV3-Object-Tracking/refs/heads/main/results/butterfly.gif" 
                                            alt="Butterfly GIF" 
                                            style="max-width:250px; border-radius:12px;"
                                        >
                                        <p style="font-size: 14px; color: gray;">Butterfly tracking example</p>
                                    </div>

                                </div>
                                
                                <p>What is this thing is doing:</p>

                                <ol>
                                <li>Take the first frame of your video.</li>
                                <li>Click the object you want to track.</li>
                                <li>DINOv3 splits the frame into patches and encodes each into a feature vector.</li>
                                <li>Compare the selected patch with patches in future frames using cosine similarity.</li>
                                <li>Boom üí•- you get a similarity heatmap (more orange = more likely your object).</li>
                                </ol>
                                
                                <div style="text-align: center;">
                                    <img 
                                        src="https://raw.githubusercontent.com/Vitgracer/DinoV3-Object-Tracking/refs/heads/main/resources/how_it_works.png"
                                        alt="How it works" 
                                        style="max-width:100%; border-radius:12px;"
                                    >
                                    <p style="font-size: 14px; color: gray;">Basic demo example.</p>
                                </div>
                                

                                <p>
                                This works because DINOv3's patch-level features are <b>dense, consistent, and spatially precise</b>.  
                                No fine-tuning. No magic tricks. Just pure SSL power in action.  
                                </p>
                                <p>
                                Era of ResNet is over, <b>Press F</b> ü´°
                                </p>

                                <p>
                                üîó Here is the repo: <a href="https://github.com/Vitgracer/DinoV3-Object-Tracking" target="_blank">Object Tracking GitHub</a>
                                </p>

                                <h2 style="margin-top:2rem;margin-bottom:1rem;">The Final Words</h2>
                                <p>
                                DINOv3 is more than just a research flex.  
                                It shows that with talent of researchers and involved scientists SSL can beat supervised methods while staying general-purpose.  
                                From dense tasks to global embeddings, from 7B giants to edge-friendly distilled models, it sets a new bar for foundation vision models.
                                </p>

                                <p>
                                üëâ Big thanks to the authors of  <a href="https://arxiv.org/abs/2508.10104" target="_blank">DINOv3</a> for this masterpiece!   
                                They proved that dinosaurs aren't extinct. They're running your vision models now. ü¶ñ
                                </p>

                            <!-- Post Footer -->
                            <div class="d-flex justify-content-between text-muted">
                                <span>Published on August 30, 2025</span>
                                <span>Author: Vitaly</span>
                            </div>
                        </div>
                    </div>
                </div>
            </section>
        </main>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js"></script>
        
        <!-- Prism.js CSS + JS for code highlighting -->
        <!-- <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" /> -->
        <!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script> -->
        <!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script> -->

        <!-- LaTeX rendering -->
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    </body>
</html>